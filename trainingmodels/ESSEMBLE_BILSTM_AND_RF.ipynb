{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60123041-c067-47b9-95f7-fae750ac4ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1366/1366 [==============================] - 31s 19ms/step - loss: 0.2602 - val_loss: 0.1582\n",
      "Epoch 2/100\n",
      "1366/1366 [==============================] - 26s 19ms/step - loss: 0.1638 - val_loss: 0.1590\n",
      "Epoch 3/100\n",
      "1366/1366 [==============================] - 25s 19ms/step - loss: 0.1636 - val_loss: 0.1623\n",
      "Epoch 4/100\n",
      "1366/1366 [==============================] - 26s 19ms/step - loss: 0.1634 - val_loss: 0.1587\n",
      "Epoch 5/100\n",
      "1366/1366 [==============================] - 26s 19ms/step - loss: 0.1638 - val_loss: 0.1581\n",
      "Epoch 6/100\n",
      "1366/1366 [==============================] - 25s 19ms/step - loss: 0.1627 - val_loss: 0.1583\n",
      "Epoch 7/100\n",
      "1366/1366 [==============================] - 26s 19ms/step - loss: 0.1628 - val_loss: 0.1618\n",
      "Epoch 8/100\n",
      "1366/1366 [==============================] - 26s 19ms/step - loss: 0.1623 - val_loss: 0.1578\n",
      "Epoch 9/100\n",
      "1366/1366 [==============================] - 26s 19ms/step - loss: 0.1626 - val_loss: 0.1586\n",
      "Epoch 10/100\n",
      "1366/1366 [==============================] - 26s 19ms/step - loss: 0.1625 - val_loss: 0.1582\n",
      "Epoch 11/100\n",
      "1366/1366 [==============================] - 26s 19ms/step - loss: 0.1621 - val_loss: 0.1585\n",
      "Epoch 12/100\n",
      "1366/1366 [==============================] - 26s 19ms/step - loss: 0.1619 - val_loss: 0.1578\n",
      "Epoch 13/100\n",
      "1366/1366 [==============================] - 26s 19ms/step - loss: 0.1620 - val_loss: 0.1590\n",
      "Epoch 14/100\n",
      "1366/1366 [==============================] - 26s 19ms/step - loss: 0.1623 - val_loss: 0.1585\n",
      "Epoch 15/100\n",
      "1366/1366 [==============================] - 25s 18ms/step - loss: 0.1621 - val_loss: 0.1585\n",
      "Epoch 16/100\n",
      "1366/1366 [==============================] - 25s 18ms/step - loss: 0.1620 - val_loss: 0.1583\n",
      "Epoch 17/100\n",
      "1366/1366 [==============================] - 25s 19ms/step - loss: 0.1618 - val_loss: 0.1599\n",
      "        date  store_no   turnover\n",
      "0 2023-12-31      2607  89.610743\n",
      "1 2024-01-01      2607  89.610743\n",
      "2 2024-01-02      2607  89.610743\n",
      "3 2024-01-03      2607  89.610743\n",
      "4 2024-01-04      2607  89.610743\n",
      "RMSE: 38.97\n",
      "Mean Turnover: 132.74\n",
      "Error Percentage: 29.36%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional, LSTM, Dense\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load and preprocess data\n",
    "store_data = pd.read_feather('store_data.feather')\n",
    "store_data.to_csv(\"store_data.csv\", index=False)\n",
    "\n",
    "# Remove rows from 2021 (COVID time)\n",
    "store_data = store_data[store_data['date'] >= '2023-01-01']\n",
    "\n",
    "# Ensure 'date' is in datetime format\n",
    "store_data['date'] = pd.to_datetime(store_data['date'])\n",
    "\n",
    "# One-hot encoding for region\n",
    "store_data = pd.get_dummies(data=store_data, columns=[\"region\"], drop_first=True)\n",
    "\n",
    "# Convert boolean columns to binary\n",
    "def convert_boolean_to_binary(dummy):\n",
    "    for column in dummy.select_dtypes(include='bool').columns:\n",
    "        dummy[column] = dummy[column].astype(int)\n",
    "    return dummy\n",
    "\n",
    "store_data = convert_boolean_to_binary(store_data)\n",
    "\n",
    "# Add season feature\n",
    "store_data = store_data.sort_values(by='date')\n",
    "store_data['year'] = store_data['date'].dt.year\n",
    "store_data['month'] = store_data['date'].dt.month\n",
    "store_data['week'] = store_data['date'].dt.isocalendar().week\n",
    "\n",
    "def get_season_from_month(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Fall'\n",
    "\n",
    "store_data['season'] = store_data['month'].apply(get_season_from_month)\n",
    "\n",
    "# Log transform the target variable to stabilize variance\n",
    "store_data['turnover'] = np.log1p(store_data['turnover'])\n",
    "\n",
    "# Remove rows with any NaN values in the dataframe\n",
    "store_data.dropna(inplace=True)\n",
    "\n",
    "# Feature selection\n",
    "features = ['latitude', 'store_area', 'competitor_count', 'footfall', \n",
    "            'avg_temperature', 'precipitation_mm', 'wind_direction_degrees', \n",
    "            'peak_wind_gust_kmh', 'region_Bayern', 'region_Berlin', \n",
    "            'region_Brandenburg', 'region_Bremen', 'region_Hamburg', 'region_Hessen', \n",
    "            'region_Mecklenburg-vorpommern', 'region_Niedersachsen', 'region_Nordrhein-westfalen', \n",
    "            'region_Rheinland-pfalz', 'region_Saarland', 'region_Sachsen', 'region_Sachsen-anhalt', \n",
    "            'region_Schleswig-holstein', 'region_Thüringen', 'season', 'year', 'month', 'week']\n",
    "target = 'turnover'\n",
    "\n",
    "x = store_data[features]\n",
    "y = store_data[target]\n",
    "\n",
    "# Split data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize and one-hot encode\n",
    "numerical_features = ['latitude', 'store_area', 'competitor_count', 'footfall', 'avg_temperature', \n",
    "                      'precipitation_mm', 'wind_direction_degrees', 'peak_wind_gust_kmh', \n",
    "                      'year', 'month', 'week']\n",
    "categorical_features = ['region_Bayern', 'region_Berlin', 'region_Brandenburg', 'region_Bremen', \n",
    "                        'region_Hamburg', 'region_Hessen', 'region_Mecklenburg-vorpommern', 'region_Niedersachsen', \n",
    "                        'region_Nordrhein-westfalen', 'region_Rheinland-pfalz', 'region_Saarland', 'region_Sachsen', \n",
    "                        'region_Sachsen-anhalt', 'region_Schleswig-holstein', 'region_Thüringen', 'season']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', MinMaxScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ],\n",
    "    sparse_threshold=0\n",
    ")\n",
    "\n",
    "x_train_processed = preprocessor.fit_transform(x_train)\n",
    "x_test_processed = preprocessor.transform(x_test)\n",
    "\n",
    "# Reshape for LSTM/BiLSTM\n",
    "def reshape_for_lstm(x_data, timesteps=10):\n",
    "    x_reshaped = []\n",
    "    for i in range(len(x_data) - timesteps):\n",
    "        x_reshaped.append(x_data[i:i+timesteps])\n",
    "    return np.array(x_reshaped)\n",
    "\n",
    "timesteps = 10\n",
    "x_train_lstm = reshape_for_lstm(x_train_processed, timesteps)\n",
    "x_test_lstm = reshape_for_lstm(x_test_processed, timesteps)\n",
    "y_train_lstm = y_train.iloc[timesteps:].values\n",
    "y_test_lstm = y_test.iloc[timesteps:].values\n",
    "\n",
    "# Build and train BiLSTM model with EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "bilstm_model = Sequential([\n",
    "    Bidirectional(LSTM(50, return_sequences=False), input_shape=(x_train_lstm.shape[1], x_train_lstm.shape[2])),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "bilstm_model.compile(optimizer=Adam(), loss='mse')\n",
    "bilstm_model.fit(x_train_lstm, y_train_lstm, epochs=100, batch_size=32, \n",
    "                 validation_data=(x_test_lstm, y_test_lstm), callbacks=[early_stopping], verbose=1)  # Keep verbose output\n",
    "\n",
    "# Train Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(x_train_processed, y_train)\n",
    "\n",
    "# Generate future predictions for all stores (next 14 days)\n",
    "future_predictions = []\n",
    "\n",
    "for store_id in store_data['store_no'].unique():\n",
    "    latest_data = store_data[store_data['store_no'] == store_id].iloc[-1:]\n",
    "    latest_data = latest_data.drop(['date', 'turnover'], axis=1)\n",
    "    latest_data_processed = preprocessor.transform(latest_data[features])\n",
    "\n",
    "    if latest_data_processed.shape[0] < timesteps:\n",
    "        latest_data_processed = np.tile(latest_data_processed, (timesteps, 1))\n",
    "        latest_data_processed = latest_data_processed[-timesteps:]\n",
    "\n",
    "    latest_data_lstm = np.reshape(latest_data_processed, (1, timesteps, latest_data_processed.shape[1]))\n",
    "    bilstm_future_preds = bilstm_model.predict(latest_data_lstm, verbose=0)  # Suppress verbose output for prediction\n",
    "    rf_future_preds = rf_model.predict(latest_data_processed)\n",
    "    ensemble_future_preds = (bilstm_future_preds.flatten() + rf_future_preds) / 2\n",
    "    future_turnover_ensemble = np.expm1(ensemble_future_preds.flatten())\n",
    "    future_turnover_ensemble = np.tile(future_turnover_ensemble, 14)\n",
    "\n",
    "    future_dates = pd.date_range(start=store_data['date'].max() + pd.Timedelta(days=1), periods=14, freq='D')\n",
    "    future_df = pd.DataFrame({\n",
    "        'store_no': [store_id] * 14,\n",
    "        'date': future_dates,\n",
    "        'Predicted Turnover (Ensemble)': future_turnover_ensemble[:14]\n",
    "    })\n",
    "\n",
    "    future_predictions.append(future_df)\n",
    "\n",
    "# Combine all store predictions into one DataFrame\n",
    "future_predictions_df = pd.concat(future_predictions, ignore_index=True)\n",
    "\n",
    "# Ensure columns are separate and properly formatted\n",
    "future_predictions_df = future_predictions_df[['date', 'store_no', 'Predicted Turnover (Ensemble)']]\n",
    "\n",
    "# Rename columns to the desired format\n",
    "future_predictions_df.rename(columns={'Predicted Turnover (Ensemble)': 'turnover'}, inplace=True)\n",
    "\n",
    "# Save to Excel with proper column separation\n",
    "future_predictions_df.to_excel('future_turnover_predictions_all_stores_BILSTM_AND_RANDOM_FOREST.xlsx', index=False)\n",
    "\n",
    "# Print the first few rows of the predictions for inspection\n",
    "print(future_predictions_df.head())\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "\n",
    "# Adjust indices for Random Forest predictions\n",
    "rf_predictions_test = rf_model.predict(x_test_processed[timesteps:])  # Use aligned data for RF predictions\n",
    "\n",
    "# BiLSTM predictions\n",
    "bilstm_predictions_test = bilstm_model.predict(x_test_lstm, verbose=0).flatten()\n",
    "\n",
    "# Combine predictions from BiLSTM and Random Forest\n",
    "ensemble_predictions_test = (bilstm_predictions_test + rf_predictions_test) / 2\n",
    "\n",
    "# Reverse the log1p transformation for predictions and actuals\n",
    "ensemble_predictions_test_exp = np.expm1(ensemble_predictions_test)\n",
    "y_test_lstm_exp = np.expm1(y_test_lstm)  # Ensure y_test_lstm is used as it matches the BiLSTM alignment\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test_lstm_exp, ensemble_predictions_test_exp))\n",
    "\n",
    "# Calculate turnover mean\n",
    "turnover_mean = np.mean(y_test_lstm_exp)\n",
    "\n",
    "# Calculate error percentage\n",
    "error_percentage = (rmse / turnover_mean) * 100\n",
    "\n",
    "# Print results\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"Mean Turnover: {turnover_mean:.2f}\")\n",
    "print(f\"Error Percentage: {error_percentage:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
